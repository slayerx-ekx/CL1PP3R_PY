1
00:00:00,030 --> 00:00:04,980
today we have Elon Musk Eon thank you

2
00:00:02,100 --> 00:00:06,089
for joining us thanks having right so we

3
00:00:04,980 --> 00:00:08,790
want to spend the time today talking

4
00:00:06,089 --> 00:00:11,160
about your view of the future and what

5
00:00:08,790 --> 00:00:13,679
people should work on so to start off

6
00:00:11,160 --> 00:00:14,519
could you tell us you famously said when

7
00:00:13,679 --> 00:00:16,350
you were younger there were five

8
00:00:14,519 --> 00:00:18,180
problems that you thought were most

9
00:00:16,350 --> 00:00:20,039
important for you to work on um if you

10
00:00:18,180 --> 00:00:21,090
were 22 today what would the five

11
00:00:20,039 --> 00:00:23,699
problems that you would think about

12
00:00:21,090 --> 00:00:26,369
working on B well first of all is it I

13
00:00:23,699 --> 00:00:29,340
think if somebody is doing something

14
00:00:26,369 --> 00:00:30,539
that is useful to the rest of society I

15
00:00:29,340 --> 00:00:32,130
think that's a good thing like it

16
00:00:30,539 --> 00:00:35,040
doesn't have to change the world like

17
00:00:32,130 --> 00:00:38,640
you know if you make something that has

18
00:00:35,040 --> 00:00:41,190
high value to people and frankly even if

19
00:00:38,640 --> 00:00:46,170
it's something if it's like just a

20
00:00:41,190 --> 00:00:47,610
little game or you know the some

21
00:00:46,170 --> 00:00:49,710
improvement in photo-sharing or

22
00:00:47,610 --> 00:00:52,160
something if it if it has a small amount

23
00:00:49,710 --> 00:00:55,020
of good for a large number of people

24
00:00:52,160 --> 00:00:57,390
that's I mean I think that's that's fine

25
00:00:55,020 --> 00:01:01,530
like stuff doesn't need to be change the

26
00:00:57,390 --> 00:01:03,359
world just to be good but you know in

27
00:01:01,530 --> 00:01:04,920
terms of things that I think are most

28
00:01:03,359 --> 00:01:08,490
likely to affect the the future of

29
00:01:04,920 --> 00:01:09,840
humanity I think AI is probably the

30
00:01:08,490 --> 00:01:13,350
single biggest item in the near term

31
00:01:09,840 --> 00:01:15,299
that's likely to affect humanity so it's

32
00:01:13,350 --> 00:01:19,710
very important that we have the advent

33
00:01:15,299 --> 00:01:23,909
of AI in a good way but that is

34
00:01:19,710 --> 00:01:25,650
something that if you if you could look

35
00:01:23,909 --> 00:01:28,070
into a crucible and enter the future you

36
00:01:25,650 --> 00:01:30,619
would like you would like that outcome

37
00:01:28,070 --> 00:01:33,810
because it is something that could go

38
00:01:30,619 --> 00:01:36,869
could go wrong and as we've talked about

39
00:01:33,810 --> 00:01:41,280
many times and so we really need to make

40
00:01:36,869 --> 00:01:44,180
sure it goes right that's that's I think

41
00:01:41,280 --> 00:01:46,290
AI working on ai and making sure it's

42
00:01:44,180 --> 00:01:49,020
great future that's that's the most

43
00:01:46,290 --> 00:01:52,950
important thing I think right now the

44
00:01:49,020 --> 00:01:55,470
most pressing item sec then obviously I

45
00:01:52,950 --> 00:01:59,360
think Institute with with genetics if

46
00:01:55,470 --> 00:02:03,719
you can actually solve genetic diseases

47
00:01:59,360 --> 00:02:05,250
if you can promote dementia or

48
00:02:03,719 --> 00:02:07,350
Alzheimer's or something like that that

49
00:02:05,250 --> 00:02:08,069
was genetic reprogramming that would be

50
00:02:07,350 --> 00:02:10,000
wonderful

51
00:02:08,069 --> 00:02:13,900
so I think this

52
00:02:10,000 --> 00:02:18,250
genetics it might be the sort of second

53
00:02:13,900 --> 00:02:20,890
most important item I think having a

54
00:02:18,250 --> 00:02:23,110
high bandwidth interface to the brain

55
00:02:20,890 --> 00:02:26,140
like we're currently bandwidth-limited

56
00:02:23,110 --> 00:02:27,580
we have a digital tertiary self in the

57
00:02:26,140 --> 00:02:30,790
form of our email capabilities like

58
00:02:27,580 --> 00:02:32,680
computers phones applications we're

59
00:02:30,790 --> 00:02:34,660
effectively superhuman but we're

60
00:02:32,680 --> 00:02:36,580
extremely bad with constraint in that

61
00:02:34,660 --> 00:02:40,750
interface between the cortex and your

62
00:02:36,580 --> 00:02:42,700
sort of that the tertiary digital form

63
00:02:40,750 --> 00:02:45,370
of yourself and helping solve that

64
00:02:42,700 --> 00:02:47,590
bandwidth constraint would be I think

65
00:02:45,370 --> 00:02:50,560
very important for the future as well so

66
00:02:47,590 --> 00:02:53,230
one of the I think most common questions

67
00:02:50,560 --> 00:02:54,610
I hear young people and bishops young

68
00:02:53,230 --> 00:02:57,750
people ask is I want to be the next to

69
00:02:54,610 --> 00:02:59,950
go musk how do I do that um obviously

70
00:02:57,750 --> 00:03:01,450
the next Elon Musk will work on very

71
00:02:59,950 --> 00:03:03,700
different things then than you did but

72
00:03:01,450 --> 00:03:05,800
what have you done or what did you do

73
00:03:03,700 --> 00:03:08,070
when you were younger that you think

74
00:03:05,800 --> 00:03:10,150
sort of set you up to have a big impact

75
00:03:08,070 --> 00:03:11,680
well I think this well I should say that

76
00:03:10,150 --> 00:03:15,580
I do not expect to be involved in all

77
00:03:11,680 --> 00:03:17,739
these things so the the the five things

78
00:03:15,580 --> 00:03:22,530
that I thought about the time in college

79
00:03:17,739 --> 00:03:24,790
still quite a long time ago 25 years ago

80
00:03:22,530 --> 00:03:28,540
you know being you know making life

81
00:03:24,790 --> 00:03:31,959
multiplanetary selling accelerating the

82
00:03:28,540 --> 00:03:35,790
transition to sustainable energy the the

83
00:03:31,959 --> 00:03:39,040
Internet broadly speaking and then

84
00:03:35,790 --> 00:03:41,530
genetics and AI I think I didn't expect

85
00:03:39,040 --> 00:03:43,959
to be involved in in all of those things

86
00:03:41,530 --> 00:03:46,180
I actually at the time in college I sort

87
00:03:43,959 --> 00:03:49,060
of thought helping with electrification

88
00:03:46,180 --> 00:03:50,860
a bit of cars which was how we start out

89
00:03:49,060 --> 00:03:53,739
and that's a that's actually what I

90
00:03:50,860 --> 00:03:56,680
worked on as an intern was advanced

91
00:03:53,739 --> 00:03:57,850
ultra capacitors with to see thick there

92
00:03:56,680 --> 00:04:00,430
would be a breakthrough relative to

93
00:03:57,850 --> 00:04:03,040
batteries for energy storage and in cars

94
00:04:00,430 --> 00:04:04,780
and then when I came out to go to

95
00:04:03,040 --> 00:04:07,060
Stanford that's what I was going to be

96
00:04:04,780 --> 00:04:09,760
doing my grad studies on is it was

97
00:04:07,060 --> 00:04:12,459
working on her best at energy storage

98
00:04:09,760 --> 00:04:15,070
technologies for electric cars and I put

99
00:04:12,459 --> 00:04:19,870
that on hold to start an Internet

100
00:04:15,070 --> 00:04:21,609
company in 95 because that that does

101
00:04:19,870 --> 00:04:23,120
seem to be like a time for particular

102
00:04:21,609 --> 00:04:26,360
technologies when there

103
00:04:23,120 --> 00:04:30,350
at a steep point in the inflection code

104
00:04:26,360 --> 00:04:32,120
and and I didn't want to you know do a

105
00:04:30,350 --> 00:04:35,120
PhD at Stanford and then and what sure

106
00:04:32,120 --> 00:04:36,949
will happen and then and I wasn't

107
00:04:35,120 --> 00:04:39,710
entirely certain that the technology I'd

108
00:04:36,949 --> 00:04:42,500
be working on would actually succeed I

109
00:04:39,710 --> 00:04:44,060
can get you can get a you know doctrine

110
00:04:42,500 --> 00:04:46,760
on many things that ultimately are not

111
00:04:44,060 --> 00:04:49,880
do not have a practical bearing on the

112
00:04:46,760 --> 00:04:51,470
world and I wanted to you know just I

113
00:04:49,880 --> 00:04:54,320
really was just trying to be useful

114
00:04:51,470 --> 00:04:55,970
that's the optimization it's like what

115
00:04:54,320 --> 00:04:58,460
do what can I do that would actually be

116
00:04:55,970 --> 00:05:01,030
useful do you think people that want to

117
00:04:58,460 --> 00:05:05,690
be useful today should get PhDs um

118
00:05:01,030 --> 00:05:08,930
mostly not so what what is the best some

119
00:05:05,690 --> 00:05:10,660
yes but mostly not how should someone

120
00:05:08,930 --> 00:05:12,260
figure out how they can be most useful

121
00:05:10,660 --> 00:05:14,810
whatever this thing is that you're

122
00:05:12,260 --> 00:05:18,380
trying to create what would what would

123
00:05:14,810 --> 00:05:20,360
be the utility Delta compared to the

124
00:05:18,380 --> 00:05:22,430
current state-of-the-art times how many

125
00:05:20,360 --> 00:05:24,919
people it would affect so that's why I

126
00:05:22,430 --> 00:05:27,080
think having something that has that

127
00:05:24,919 --> 00:05:29,330
that has a mix makes a big difference

128
00:05:27,080 --> 00:05:31,220
but effects a sort of small to moderate

129
00:05:29,330 --> 00:05:32,840
number of people is great as is

130
00:05:31,220 --> 00:05:34,700
something that makes even a small

131
00:05:32,840 --> 00:05:36,979
difference but but affects a vast number

132
00:05:34,700 --> 00:05:39,470
of people like the area yeah on you know

133
00:05:36,979 --> 00:05:41,000
under the yeah exactly the area under

134
00:05:39,470 --> 00:05:42,380
the curve is would actually be roughly

135
00:05:41,000 --> 00:05:46,430
similar for those two things so it's

136
00:05:42,380 --> 00:05:49,610
actually really about yeah just trying

137
00:05:46,430 --> 00:05:51,500
to be useful and matter when you're

138
00:05:49,610 --> 00:05:52,970
trying to estimate probability of

139
00:05:51,500 --> 00:05:54,530
success so you say something will be

140
00:05:52,970 --> 00:05:56,389
really useful good area under the curve

141
00:05:54,530 --> 00:05:58,760
I guess to use the example of SpaceX

142
00:05:56,389 --> 00:05:59,780
mmm-hmm when you made the NGO decision

143
00:05:58,760 --> 00:06:01,400
that you were actually going to do that

144
00:05:59,780 --> 00:06:03,979
this was kind of a very crazy thing at

145
00:06:01,400 --> 00:06:06,590
the time very crazy there shortly yeah

146
00:06:03,979 --> 00:06:07,880
I'm not shy about saying that but I kind

147
00:06:06,590 --> 00:06:11,720
of agree I agreed with them that it was

148
00:06:07,880 --> 00:06:16,160
quite crazy crazy if if the objective

149
00:06:11,720 --> 00:06:20,300
was to achieve the best risk adjusted

150
00:06:16,160 --> 00:06:21,380
return sliding our company is insane but

151
00:06:20,300 --> 00:06:24,250
that was not that was not my objective

152
00:06:21,380 --> 00:06:27,229
why I I'd totally come to the conclusion

153
00:06:24,250 --> 00:06:28,940
that if something didn't happen to

154
00:06:27,229 --> 00:06:32,270
improve ROC technology would be stuck on

155
00:06:28,940 --> 00:06:34,789
earth forever and and the big aerospace

156
00:06:32,270 --> 00:06:36,339
companies had just had no interest in

157
00:06:34,789 --> 00:06:38,379
radical innovation

158
00:06:36,339 --> 00:06:41,710
all they wanted to do is try to make

159
00:06:38,379 --> 00:06:44,110
their old technology slightly better

160
00:06:41,710 --> 00:06:46,029
every year and in fact sometimes we

161
00:06:44,110 --> 00:06:47,949
would actually get worse and

162
00:06:46,029 --> 00:06:50,860
particularly in Rockets is pretty bad

163
00:06:47,949 --> 00:06:53,619
like the in 69 we were able to go to the

164
00:06:50,860 --> 00:06:54,789
moon with a Saturn 5 and then the space

165
00:06:53,619 --> 00:06:55,869
shuttle could only take people to

166
00:06:54,789 --> 00:06:57,520
low-earth orbit and then the Space

167
00:06:55,869 --> 00:07:01,419
Shuttle retired and that trend is

168
00:06:57,520 --> 00:07:03,550
basically trends to zero it feels I

169
00:07:01,419 --> 00:07:05,050
think technology just automatically gets

170
00:07:03,550 --> 00:07:07,389
better over year but I actually doesn't

171
00:07:05,050 --> 00:07:09,189
it only gets better if smart people work

172
00:07:07,389 --> 00:07:11,589
work like crazy to make it better

173
00:07:09,189 --> 00:07:15,729
that's how any technology actually gets

174
00:07:11,589 --> 00:07:17,229
better and by itself technology if

175
00:07:15,729 --> 00:07:20,529
people don't work and it actually will

176
00:07:17,229 --> 00:07:22,479
decline you can look at the history of

177
00:07:20,529 --> 00:07:25,180
civilizations many civilizations and

178
00:07:22,479 --> 00:07:26,710
look at say ancient Egypt were they able

179
00:07:25,180 --> 00:07:28,839
to pull these incredible pyramids and

180
00:07:26,710 --> 00:07:33,279
then they basically forgot how to hold

181
00:07:28,839 --> 00:07:34,990
permit and and then even hieroglyphics

182
00:07:33,279 --> 00:07:36,999
they forgot how to read hydrocal

183
00:07:34,990 --> 00:07:38,319
hieroglyphics so we look at Rome and how

184
00:07:36,999 --> 00:07:40,209
they will to look to build these

185
00:07:38,319 --> 00:07:41,589
incredible roadways and aqueducts and

186
00:07:40,209 --> 00:07:45,520
indoor plumbing and they forgot how to

187
00:07:41,589 --> 00:07:49,719
do all of those things and there are

188
00:07:45,520 --> 00:07:55,800
many such examples in history so I I

189
00:07:49,719 --> 00:07:57,610
think choice bear in mind that you know

190
00:07:55,800 --> 00:08:01,360
entropy is not on your side

191
00:07:57,610 --> 00:08:03,099
yeah one thing I really like about you

192
00:08:01,360 --> 00:08:04,899
is you are unusually fearless and

193
00:08:03,099 --> 00:08:05,800
willing to go in the face of other

194
00:08:04,899 --> 00:08:07,839
people telling you something that's

195
00:08:05,800 --> 00:08:09,490
crazy and I know a lot of pretty crazy

196
00:08:07,839 --> 00:08:10,959
people you still stand out where does

197
00:08:09,490 --> 00:08:13,029
that come from or how do you think about

198
00:08:10,959 --> 00:08:14,680
making a decision when everyone tells

199
00:08:13,029 --> 00:08:16,499
you this is a crazy idea where do you

200
00:08:14,680 --> 00:08:18,399
get the internal strength to do that

201
00:08:16,499 --> 00:08:19,990
well first of all I'd say I actually

202
00:08:18,399 --> 00:08:23,529
think I feel

203
00:08:19,990 --> 00:08:25,990
feel fair quite strongly so it's not as

204
00:08:23,529 --> 00:08:29,559
though I just have the absence of fear

205
00:08:25,990 --> 00:08:32,079
I've I feel it quite strongly but there

206
00:08:29,559 --> 00:08:34,599
are times when something is important

207
00:08:32,079 --> 00:08:36,360
enough you believe in it enough that you

208
00:08:34,599 --> 00:08:38,560
do it in spite of the fear

209
00:08:36,360 --> 00:08:41,289
so speaking of important things like

210
00:08:38,560 --> 00:08:43,569
people shouldn't think I I I should if

211
00:08:41,289 --> 00:08:46,149
you think well I feel fear about this

212
00:08:43,569 --> 00:08:48,399
and therefore I shouldn't do it it's

213
00:08:46,149 --> 00:08:49,029
normal to be to feel fair like you'd

214
00:08:48,399 --> 00:08:50,529
have to

215
00:08:49,029 --> 00:08:54,699
definitely something mentally wrong you

216
00:08:50,529 --> 00:08:56,439
should feel fair so you just feel it and

217
00:08:54,699 --> 00:08:58,600
let the importance of it drive you to do

218
00:08:56,439 --> 00:08:59,769
it anyway yeah you know actually

219
00:08:58,600 --> 00:09:02,829
something that can be helpful as

220
00:08:59,769 --> 00:09:05,370
fatalism some degree if you just think

221
00:09:02,829 --> 00:09:10,600
it's just accept the probabilities then

222
00:09:05,370 --> 00:09:12,639
that diminishes fear so my starting

223
00:09:10,600 --> 00:09:16,480
SpaceX I thought the odds of success

224
00:09:12,639 --> 00:09:18,670
were less than 10% and I just accepted

225
00:09:16,480 --> 00:09:22,990
that actually probably I would just lose

226
00:09:18,670 --> 00:09:24,910
lose everything but that maybe would

227
00:09:22,990 --> 00:09:27,420
make some progress if we could just move

228
00:09:24,910 --> 00:09:29,500
the ball forward even if we died maybe

229
00:09:27,420 --> 00:09:31,000
some other company could pick up the

230
00:09:29,500 --> 00:09:35,009
baton and move and keep moving it

231
00:09:31,000 --> 00:09:37,839
forward so that were still do some good

232
00:09:35,009 --> 00:09:39,160
yeah same with Tesla I thought your odds

233
00:09:37,839 --> 00:09:41,470
of a car company succeeding were

234
00:09:39,160 --> 00:09:42,879
extremely low what do you think the odds

235
00:09:41,470 --> 00:09:46,629
of the Mars colony are at this point

236
00:09:42,879 --> 00:09:49,870
today well um

237
00:09:46,629 --> 00:09:51,519
oddly enough I actually think they're

238
00:09:49,870 --> 00:09:55,720
pretty good

239
00:09:51,519 --> 00:09:58,269
so like when can I go okay at this point

240
00:09:55,720 --> 00:09:59,620
I am certain there is a way I'm certain

241
00:09:58,269 --> 00:10:01,449
that success is one of the possible

242
00:09:59,620 --> 00:10:03,399
outcomes for establishing a

243
00:10:01,449 --> 00:10:05,620
self-sustaining moss colony in fact

244
00:10:03,399 --> 00:10:06,819
growing lost colony I'm certain that

245
00:10:05,620 --> 00:10:10,269
that is possible

246
00:10:06,819 --> 00:10:12,129
whereas until maybe a few years ago I

247
00:10:10,269 --> 00:10:14,410
was not sure that success was even one

248
00:10:12,129 --> 00:10:15,610
of the possible outcomes it's a

249
00:10:14,410 --> 00:10:18,790
meaningful number of people going to

250
00:10:15,610 --> 00:10:20,410
Mars I think this is potentially

251
00:10:18,790 --> 00:10:22,509
something that can be accomplished in

252
00:10:20,410 --> 00:10:27,579
about 10 years

253
00:10:22,509 --> 00:10:29,589
maybe sooner I mean maybe 9 years I need

254
00:10:27,579 --> 00:10:31,360
to make sure that SpaceX doesn't die

255
00:10:29,589 --> 00:10:33,550
between now and then and that I don't

256
00:10:31,360 --> 00:10:35,920
die or if I do die that someone takes

257
00:10:33,550 --> 00:10:37,959
over who will continue that you

258
00:10:35,920 --> 00:10:41,170
shouldn't go on the first launch yeah

259
00:10:37,959 --> 00:10:43,300
exactly like the first launch will be a

260
00:10:41,170 --> 00:10:46,000
robotic anyway so I want to go except

261
00:10:43,300 --> 00:10:48,540
for that internet latency yeah they were

262
00:10:46,000 --> 00:10:50,949
at latency to be pretty significant I

263
00:10:48,540 --> 00:10:53,410
Mars is roughly 12 light minutes from

264
00:10:50,949 --> 00:10:55,269
the Sun and Earth is 8 light minutes so

265
00:10:53,410 --> 00:10:57,360
closest approach Mazdas 4 light minutes

266
00:10:55,269 --> 00:10:59,170
away that furthest approaches 20 a

267
00:10:57,360 --> 00:11:00,790
little more because you have to you

268
00:10:59,170 --> 00:11:01,660
can't sort of talk directly through the

269
00:11:00,790 --> 00:11:04,360
Sun

270
00:11:01,660 --> 00:11:07,000
speaking of really important problems um

271
00:11:04,360 --> 00:11:09,820
AI so you have been outspoken about AI

272
00:11:07,000 --> 00:11:11,710
um could you talk about what you think

273
00:11:09,820 --> 00:11:14,800
the positive future for AI looks like

274
00:11:11,710 --> 00:11:17,770
and how we get there okay I mean I do

275
00:11:14,800 --> 00:11:22,630
want to emphasize that um this is not

276
00:11:17,770 --> 00:11:25,420
really something that I advocate or this

277
00:11:22,630 --> 00:11:29,500
is not prescriptive this is simply

278
00:11:25,420 --> 00:11:31,660
pretty hopefully predictive as you look

279
00:11:29,500 --> 00:11:34,060
so I'm Sayla well like this is something

280
00:11:31,660 --> 00:11:36,580
that I want to occur instead of so this

281
00:11:34,060 --> 00:11:42,460
I mean I think that probably is the best

282
00:11:36,580 --> 00:11:45,280
of the available alternatives the best

283
00:11:42,460 --> 00:11:46,510
of the available alternatives that I can

284
00:11:45,280 --> 00:11:49,330
come up with and maybe somebody else can

285
00:11:46,510 --> 00:11:52,720
come up with a better approach or better

286
00:11:49,330 --> 00:11:55,000
outcome is that we achieve

287
00:11:52,720 --> 00:11:59,800
democratization of AI technology meaning

288
00:11:55,000 --> 00:12:01,810
that no one company or small set of

289
00:11:59,800 --> 00:12:04,480
individuals has control over advanced AI

290
00:12:01,810 --> 00:12:06,510
technology like that that's very

291
00:12:04,480 --> 00:12:09,310
dangerous

292
00:12:06,510 --> 00:12:11,740
it could also get stolen by somebody bad

293
00:12:09,310 --> 00:12:14,140
you know like some evil dictator or

294
00:12:11,740 --> 00:12:15,850
country could send their intelligence

295
00:12:14,140 --> 00:12:17,740
agency to go steal it and gain control

296
00:12:15,850 --> 00:12:21,460
it just becomes a very unstable

297
00:12:17,740 --> 00:12:25,780
situation I think if you've got any any

298
00:12:21,460 --> 00:12:27,280
incredibly powerful AI you just don't

299
00:12:25,780 --> 00:12:29,500
know who's who's going to control that

300
00:12:27,280 --> 00:12:30,910
so it's not as I think that the risk is

301
00:12:29,500 --> 00:12:32,950
that the AI would develop a will of its

302
00:12:30,910 --> 00:12:36,600
own right off the bat I think it's more

303
00:12:32,950 --> 00:12:40,750
that's the consumers that some someone

304
00:12:36,600 --> 00:12:42,070
may use it in a way that is bad or and

305
00:12:40,750 --> 00:12:43,420
even if they weren't going to use in a

306
00:12:42,070 --> 00:12:44,800
way that's bad but somebody could take

307
00:12:43,420 --> 00:12:47,620
it from them and use it in a way that's

308
00:12:44,800 --> 00:12:49,000
bad that that I think is quite a big

309
00:12:47,620 --> 00:12:50,950
danger so I think we must have

310
00:12:49,000 --> 00:12:54,280
democratization of AI technology make it

311
00:12:50,950 --> 00:12:57,490
widely available and that's you know the

312
00:12:54,280 --> 00:13:00,990
reason that obviously you mean the rest

313
00:12:57,490 --> 00:13:05,950
the team you know created open AI was to

314
00:13:00,990 --> 00:13:08,200
help with the democracy our a AI

315
00:13:05,950 --> 00:13:10,990
technology so it doesn't get

316
00:13:08,200 --> 00:13:13,350
concentrated in the hands of a few and

317
00:13:10,990 --> 00:13:15,700
but then that of course that needs to be

318
00:13:13,350 --> 00:13:18,100
combined with

319
00:13:15,700 --> 00:13:23,620
solving the high bandwidth interface to

320
00:13:18,100 --> 00:13:27,520
the cortex humans are so slow humans are

321
00:13:23,620 --> 00:13:29,290
so slow yes exactly but you know we

322
00:13:27,520 --> 00:13:30,580
already have a situation in our brain

323
00:13:29,290 --> 00:13:34,209
where we've got the cortex and limbic

324
00:13:30,580 --> 00:13:37,390
system and the limbic system is kind of

325
00:13:34,209 --> 00:13:40,050
a mess that's the primitive brain it's

326
00:13:37,390 --> 00:13:43,810
kind of like the your instincts and

327
00:13:40,050 --> 00:13:46,839
whatnot and then the cortex is thinking

328
00:13:43,810 --> 00:13:49,510
of a part of the brain those two seem to

329
00:13:46,839 --> 00:13:50,950
work together quite well occasionally

330
00:13:49,510 --> 00:13:53,620
your cortex and limbic system may

331
00:13:50,950 --> 00:13:54,880
disagree but they attending it works

332
00:13:53,620 --> 00:13:57,670
pretty generally works pretty well and

333
00:13:54,880 --> 00:13:59,380
it's like rare to find someone who I've

334
00:13:57,670 --> 00:14:00,910
not found someone who wishes to either

335
00:13:59,380 --> 00:14:03,399
get rid of the cortex or get rid of the

336
00:14:00,910 --> 00:14:05,830
Olympic system very true

337
00:14:03,399 --> 00:14:13,050
yeah it's that's unusual so so I think

338
00:14:05,830 --> 00:14:17,170
if we can effectively merge with AI by

339
00:14:13,050 --> 00:14:20,529
improving that the neural link between

340
00:14:17,170 --> 00:14:21,910
your cortex and the your digital

341
00:14:20,529 --> 00:14:24,370
extension yourself which already likes

342
00:14:21,910 --> 00:14:29,560
that already exists just has a bandwidth

343
00:14:24,370 --> 00:14:34,329
issue and then then effectively you

344
00:14:29,560 --> 00:14:37,209
become an AI human symbiote and and if

345
00:14:34,329 --> 00:14:39,820
that then is widespread with anyone who

346
00:14:37,209 --> 00:14:42,160
wants it can have it then we solve a

347
00:14:39,820 --> 00:14:45,100
control problem as well we don't have to

348
00:14:42,160 --> 00:14:47,459
worry about some sort of evil dictator

349
00:14:45,100 --> 00:14:50,470
AI because kind of we are the AI

350
00:14:47,459 --> 00:14:53,589
collectively that seems like the best

351
00:14:50,470 --> 00:14:55,480
outcome I can think of so you've seen

352
00:14:53,589 --> 00:14:57,459
other companies in the early days that

353
00:14:55,480 --> 00:14:59,230
start small and get really successful um

354
00:14:57,459 --> 00:15:00,490
hope I don't regret asking this on

355
00:14:59,230 --> 00:15:04,029
camera but how do you think open AI is

356
00:15:00,490 --> 00:15:05,440
going as a six month old company I taste

357
00:15:04,029 --> 00:15:06,820
you go pretty well I think we've got a

358
00:15:05,440 --> 00:15:09,610
really talented group what opening eye

359
00:15:06,820 --> 00:15:11,170
and yeah really really talented team and

360
00:15:09,610 --> 00:15:15,850
they're working hard

361
00:15:11,170 --> 00:15:19,200
open a is structured as see a 501c3

362
00:15:15,850 --> 00:15:22,060
nonprofit but you know many nonprofits

363
00:15:19,200 --> 00:15:23,020
do not have a sense of urgency it's fine

364
00:15:22,060 --> 00:15:27,700
they don't have to have a sense of

365
00:15:23,020 --> 00:15:29,110
urgency but opening ideas

366
00:15:27,700 --> 00:15:32,440
is I think people really believe in the

367
00:15:29,110 --> 00:15:37,000
mission I think it's important and it's

368
00:15:32,440 --> 00:15:41,920
about minimizing the risk of existential

369
00:15:37,000 --> 00:15:43,810
harm in the future and so I think it's

370
00:15:41,920 --> 00:15:46,390
going well I'm pretty impressed with

371
00:15:43,810 --> 00:15:48,790
what people are doing in the talent

372
00:15:46,390 --> 00:15:52,360
level and obviously we're always looking

373
00:15:48,790 --> 00:15:54,400
for great people to join we call a

374
00:15:52,360 --> 00:15:56,260
mission list of 40 people knots yes well

375
00:15:54,400 --> 00:15:58,450
well alright just a few more questions

376
00:15:56,260 --> 00:16:00,760
before we we wrap up how do you spend

377
00:15:58,450 --> 00:16:03,610
your days now like what what do you

378
00:16:00,760 --> 00:16:06,940
allocate most of your time to my time is

379
00:16:03,610 --> 00:16:11,250
mostly split what's between SpaceX and

380
00:16:06,940 --> 00:16:13,930
Tesla and of course I I try to spend

381
00:16:11,250 --> 00:16:18,130
it's a part of every week at opening I

382
00:16:13,930 --> 00:16:20,610
so I spend most I spend basically half a

383
00:16:18,130 --> 00:16:23,230
day at opening I most weeks and then and

384
00:16:20,610 --> 00:16:25,690
then I have some open enough that

385
00:16:23,230 --> 00:16:27,580
happens during the week but other than

386
00:16:25,690 --> 00:16:28,870
that it's really traceable interlaced

387
00:16:27,580 --> 00:16:31,840
like so Tesla like what is your time

388
00:16:28,870 --> 00:16:33,700
look like there yeah so it's a good

389
00:16:31,840 --> 00:16:35,740
question um I think a lot of people

390
00:16:33,700 --> 00:16:38,980
think I must spend a lot of time with

391
00:16:35,740 --> 00:16:41,170
media or on business II things but

392
00:16:38,980 --> 00:16:43,690
actually almost almost all my time like

393
00:16:41,170 --> 00:16:46,680
80% of it is spent on engineering design

394
00:16:43,690 --> 00:16:50,040
in engineering and design so it's

395
00:16:46,680 --> 00:16:54,820
developing next generation product at

396
00:16:50,040 --> 00:16:56,470
that's 80% of it you probably remember

397
00:16:54,820 --> 00:16:57,910
this a very long time ago many many

398
00:16:56,470 --> 00:16:59,830
years you took me on a tour of SpaceX

399
00:16:57,910 --> 00:17:01,450
and the most impressive thing was that

400
00:16:59,830 --> 00:17:02,620
you knew every detail of the rocket and

401
00:17:01,450 --> 00:17:04,600
every piece of engineering that went

402
00:17:02,620 --> 00:17:05,320
into it I don't think many people get

403
00:17:04,600 --> 00:17:07,030
that about you

404
00:17:05,320 --> 00:17:08,860
yeah I think a lot of people think I'm

405
00:17:07,030 --> 00:17:10,570
kind of a business person or something

406
00:17:08,860 --> 00:17:15,370
it just fine like business is fine but

407
00:17:10,570 --> 00:17:18,010
um a guy it really it's you know it was

408
00:17:15,370 --> 00:17:21,090
like it SpaceX Gwynne Shotwell was chief

409
00:17:18,010 --> 00:17:25,870
operating officer she kind of manages

410
00:17:21,090 --> 00:17:28,090
legal finance sales and kind of general

411
00:17:25,870 --> 00:17:30,040
business activity and then my time is

412
00:17:28,090 --> 00:17:33,070
almost entirely with the engineering

413
00:17:30,040 --> 00:17:35,980
team working on improving that the

414
00:17:33,070 --> 00:17:37,060
Falcon 9 and the Dragon spacecraft and

415
00:17:35,980 --> 00:17:40,350
developing the most colonial

416
00:17:37,060 --> 00:17:45,059
architecture I mean that Tesla

417
00:17:40,350 --> 00:17:47,510
it's working on the model 3 and yes I'm

418
00:17:45,059 --> 00:17:51,720
in the design studio to agree up

419
00:17:47,510 --> 00:17:54,960
happening a week dealing with its

420
00:17:51,720 --> 00:17:58,049
aesthetics and and look and feel things

421
00:17:54,960 --> 00:18:01,049
and then most of our week is just going

422
00:17:58,049 --> 00:18:03,530
through engineering of the car itself as

423
00:18:01,049 --> 00:18:06,960
well as engineering of the factory

424
00:18:03,530 --> 00:18:10,470
because the biggest epiphany I've had

425
00:18:06,960 --> 00:18:12,390
thus this year is that what really

426
00:18:10,470 --> 00:18:15,179
matters is that is the machine that

427
00:18:12,390 --> 00:18:16,620
builds the machine the factory and this

428
00:18:15,179 --> 00:18:18,600
that is at least towards my to eat

429
00:18:16,620 --> 00:18:21,240
harder than the vehicle itself it's

430
00:18:18,600 --> 00:18:22,890
amazing to watch the robots go here and

431
00:18:21,240 --> 00:18:26,610
these cars just happen

432
00:18:22,890 --> 00:18:28,470
yeah now this actually is has a

433
00:18:26,610 --> 00:18:31,710
relatively low level of automation

434
00:18:28,470 --> 00:18:33,450
compared to what the gigafactory will

435
00:18:31,710 --> 00:18:35,390
have and what model 3 will have what's

436
00:18:33,450 --> 00:18:38,159
the speed on the line of these cars

437
00:18:35,390 --> 00:18:45,169
actually average the line is incredibly

438
00:18:38,159 --> 00:18:50,990
slow it's probably about a both X and s

439
00:18:45,169 --> 00:18:53,580
it's maybe a 5 you know 5 centimeters

440
00:18:50,990 --> 00:18:55,320
per second and what can you go this is

441
00:18:53,580 --> 00:18:59,370
very slow or what would you like to get

442
00:18:55,320 --> 00:19:01,080
to I'm confident we can get to to at

443
00:18:59,370 --> 00:19:03,690
least 1 meter per second so 20-fold

444
00:19:01,080 --> 00:19:06,480
increase that would be very fast

445
00:19:03,690 --> 00:19:08,039
yeah um at least I mean I think quite a

446
00:19:06,480 --> 00:19:11,010
1 meter per second just put that in

447
00:19:08,039 --> 00:19:13,350
perspective is a slow walk or a good

448
00:19:11,010 --> 00:19:17,400
medium speed walk a fast walk could be 1

449
00:19:13,350 --> 00:19:19,289
and 1/2 meters per second and and then

450
00:19:17,400 --> 00:19:23,070
the fastest humans can run over 10

451
00:19:19,289 --> 00:19:24,840
meters per second so if we're only doing

452
00:19:23,070 --> 00:19:28,470
point zero five meters per second that's

453
00:19:24,840 --> 00:19:30,480
very slow current current flow speed and

454
00:19:28,470 --> 00:19:33,770
and at 1 meter per second you can still

455
00:19:30,480 --> 00:19:33,770
walk faster than the production line

